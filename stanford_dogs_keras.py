# -*- coding: utf-8 -*-
"""Stanford_dogs_Keras

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lu7q3sfR9cJB696AAMynNadWcFp-hEVI
"""

import os
import numpy as np
import keras
from keras import layers
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from google.colab import drive
drive.mount('/content/drive')

import os
dataset_path = '/content/drive/My Drive/Colab_Notebooks/images.tar'
extract_path = '/content/drive/My Drive/Colab_Notebooks/extracted_images'

# Create extraction path if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# extract the tar file
!tar -xvf "{dataset_path}" -C "{extract_path}"

"""**Preprocess**"""

# check if each file is valid jpg format.
# check the first 10 bytes

path = os.path.join(extract_path, 'Images')
print(path)
def is_jpg(file_path):
  num_invalid = 0
  for folder_name in os.listdir(file_path):
    folder_path = os.path.join(file_path, folder_name)
    if os.path.isdir(folder_path):  # ensure it's a directory
      for file_name in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file_name)
        if os.path.isfile(file_path):  # ensure it's a file
          try:
            with open(file_path, 'rb') as file:
              first_10_bytes = file.read(10)
              if not first_10_bytes.startswith(b'\xff\xd8'):
                num_invalid += 1
                os.remove(file_path)
          except Exception as e:
              print(f"Error processing file {file_path}: {e}")
          #finally:
          #      file.close()

  print(f'Removed {num_invalid} images.')

  return num_invalid

deleted_files = is_jpg(path)

# preprocessing data for train and test datasets
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

test_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.2)

# declare batch size and image size
batch_size = 32
image_size = (224, 224)

# create train dataset
train_ds = train_datagen.flow_from_directory(
    path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='sparse', #'categorical',
    shuffle=True,
    subset='training',
    #seed=42
    )

# create test dataset
test_ds = test_datagen.flow_from_directory(
    path,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='sparse', #'categorical',
    shuffle=True,
    subset='validation',
    #seed=42
    )

# build a model
num_classes = 120
input_shape = (180, 180, 3)

model = keras.Sequential([
    # specify the shape of the input data
    keras.layers.Input(shape=input_shape),
    # learn 32 convolutional filters with 3x3 block and ReLU activation function
    # to detect different features
    keras.layers.Conv2D(32, (3, 3), activation='relu'),
    # take max value in each 2x2 block of the precious layer
    keras.layers.MaxPooling2D((2, 2)),
    # learn 64 filters
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    # take max
    keras.layers.MaxPooling2D((2, 2)),
    # learn 128 filters
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    # take max
    keras.layers.MaxPooling2D((2, 2)),
    # reduce each feature map to a single number
    # by taking average of all elements in the feature map
    keras.layers.GlobalAveragePooling2D(),
    # randomly sets 50% of the input units to 0 at each update
    # to prevent overfitting by making the model less sensitive to specific weights
    keras.layers.Dropout(0.5),
    # fully connected layer with number of classes units
    # softmax for multi-class classification
    keras.layers.Dense(num_classes, activation='softmax')
])

model.summary()

# compile the model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# train the model
epochs = 10

history = model.fit(
    train_ds,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=test_ds
)

# evaluate the model
loss, accuracy = model.evaluate(test_ds)
print("Test loss:", loss)
print("Test accuracy:", accuracy)

